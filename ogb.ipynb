{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected, add_self_loops\n",
    "\n",
    "def load_ogb_data(data_name):\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-'+data_name) \n",
    "    graph = dataset[0]\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    graph.train_idx, graph.valid_idx, graph.test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"] \n",
    "    # get SparseTensor adj and DAD \n",
    "    adj, D_isqrt = process_adj(graph)\n",
    "    DAD, DA, AD = gen_normalized_adjs(adj, D_isqrt)\n",
    "    graph.adj = DA\n",
    "    graph.num_classes = dataset.num_classes\n",
    "    # later think about normalize feature vector x\n",
    "    return graph \n",
    "    \n",
    "def process_adj(data):\n",
    "    N = data.num_nodes\n",
    "    data.edge_index = to_undirected(data.edge_index, N)\n",
    "    row, col = data.edge_index\n",
    "    adj = SparseTensor(row=row, col=col, sparse_sizes=(N, N))\n",
    "    # self loop \n",
    "    adj = adj.set_diag()  \n",
    "    \n",
    "    deg = adj.sum(dim=1).to(torch.float)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    return adj, deg_inv_sqrt\n",
    "\n",
    "def gen_normalized_adjs(adj, D_isqrt):\n",
    "    DAD = D_isqrt.view(-1,1)*adj*D_isqrt.view(1,-1)\n",
    "    DA = D_isqrt.view(-1,1) * D_isqrt.view(-1,1)*adj\n",
    "    AD = adj*D_isqrt.view(1,-1) * D_isqrt.view(1,-1)\n",
    "    return DAD, DA, AD\n",
    "\n",
    "def power_method_with_beta(A, x, y, alpha=1, beta=0.1, t=50):\n",
    "    # here y should only include training labels, nxc one hot\n",
    "    inv_phi_times_x = x\n",
    "    yyt_normalizer = y.matmul(y.sum(dim=0).view(-1,1)) + 1e-6\n",
    "    for _ in range(t):\n",
    "        part1 = A.matmul(inv_phi_times_x)\n",
    "        if beta > 0:\n",
    "            part2 = y.matmul(y.t().matmul(inv_phi_times_x))/yyt_normalizer\n",
    "            part1 = (1-beta)*part1 + beta*part2\n",
    "        inv_phi_times_x = alpha/(1+alpha)*part1 + 1/(1+alpha)*x\n",
    "    return inv_phi_times_x\n",
    "    \n",
    "def gpca(data, nhid=64, nlayer=1, alpha=1., beta=0, act=nn.Identity()):\n",
    "    \"\"\"\n",
    "    Currently needs the help of SparseTensor package. \n",
    "    TODO:\n",
    "        1. Extend to supervised GPCA: when beta> 0 we use supervised GPCA [different from Leman's] [Done]\n",
    "        2. Test add some non-linearity\n",
    "        3. Adapt to torch_geometric datasets\n",
    "        4. Study different normalization of x and A\n",
    "        5. Think about how to do this batch-wise, which is important for initialization,\n",
    "           also when can not fit into GPU\n",
    "    Inputs:\n",
    "        data.x - torch.Tensor\n",
    "        data.y - torch.Tensor\n",
    "        data.adj - torch_sparse.SparseTensor\n",
    "    Output:\n",
    "        x - new embeddings after gpca\n",
    "    \"\"\"\n",
    "    N = data.num_nodes\n",
    "    A = data.adj#.to_torch_sparse_coo_tensor()\n",
    "    x = data.x    \n",
    "    y_train = SparseTensor(row=data.train_idx, col=data.y.squeeze()[data.train_idx], \n",
    "                           sparse_sizes=(data.num_nodes, data.num_classes)) # one hot \n",
    "    for _ in range(nlayer):\n",
    "        x = x - x.mean(dim=0)\n",
    "#         x = x / (x.std(dim=0)+1e-6) # standarize to test \n",
    "#         pre_x = x\n",
    "        inv_phi_times_x = power_method_with_beta(data.adj, x, y_train, alpha, beta)\n",
    "        eig_val, eig_vec = torch.symeig(x.t().mm(inv_phi_times_x), eigenvectors=True)\n",
    "       # weight = eig_vec[:,-nhid:] #when nhid is large than previous hidden size, we need to sample some eigenvectors.\n",
    "        weight = torch.cat([eig_vec[:,-nhid//2:], -eig_vec[:,-nhid//2:]], dim=-1)\n",
    "        x = inv_phi_times_x.mm(weight) \n",
    "        x = act(x)\n",
    "#         x += pre_x\n",
    "    return x\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.transform = transform\n",
    "        self.x = x\n",
    "        self.y = y.squeeze()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | linear | Linear | 5.2 K \n",
      "----------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████▌  | 178/237 [00:00<00:00, 259.89it/s, loss=1.37, v_num=6, train_ce_loss=1.38]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 237/237 [00:00<00:00, 251.78it/s, loss=1.37, v_num=6, train_ce_loss=1.38, val_ce_loss=1.43, val_acc=0.657]\n",
      "Epoch 1:  75%|███████▌  | 178/237 [00:00<00:00, 254.47it/s, loss=1.17, v_num=6, train_ce_loss=1.24, val_ce_loss=1.43, val_acc=0.657]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 237/237 [00:00<00:00, 253.23it/s, loss=1.17, v_num=6, train_ce_loss=1.24, val_ce_loss=1.2, val_acc=0.676] \n",
      "Epoch 2:  75%|███████▌  | 178/237 [00:00<00:00, 254.68it/s, loss=1.11, v_num=6, train_ce_loss=1.01, val_ce_loss=1.2, val_acc=0.676]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 237/237 [00:00<00:00, 252.25it/s, loss=1.11, v_num=6, train_ce_loss=1.01, val_ce_loss=1.14, val_acc=0.68]\n",
      "Epoch 3:  75%|███████▌  | 178/237 [00:00<00:00, 257.26it/s, loss=1.1, v_num=6, train_ce_loss=1.21, val_ce_loss=1.14, val_acc=0.68]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 237/237 [00:00<00:00, 250.95it/s, loss=1.1, v_num=6, train_ce_loss=1.21, val_ce_loss=1.12, val_acc=0.682]\n",
      "Epoch 4:  75%|███████▌  | 178/237 [00:00<00:00, 262.00it/s, loss=1.08, v_num=6, train_ce_loss=1.09, val_ce_loss=1.12, val_acc=0.682] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 237/237 [00:00<00:00, 258.18it/s, loss=1.08, v_num=6, train_ce_loss=1.09, val_ce_loss=1.1, val_acc=0.684] \n",
      "Epoch 5:  75%|███████▌  | 178/237 [00:00<00:00, 261.47it/s, loss=1.07, v_num=6, train_ce_loss=1.1, val_ce_loss=1.1, val_acc=0.684]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 237/237 [00:00<00:00, 258.13it/s, loss=1.07, v_num=6, train_ce_loss=1.1, val_ce_loss=1.09, val_acc=0.684]\n",
      "Epoch 6:  75%|███████▌  | 178/237 [00:00<00:00, 254.93it/s, loss=1.07, v_num=6, train_ce_loss=1.18, val_ce_loss=1.09, val_acc=0.684] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 237/237 [00:00<00:00, 254.02it/s, loss=1.07, v_num=6, train_ce_loss=1.18, val_ce_loss=1.09, val_acc=0.685]\n",
      "Epoch 7:  75%|███████▌  | 178/237 [00:00<00:00, 256.45it/s, loss=1.05, v_num=6, train_ce_loss=1.03, val_ce_loss=1.09, val_acc=0.685] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 237/237 [00:00<00:00, 251.22it/s, loss=1.05, v_num=6, train_ce_loss=1.03, val_ce_loss=1.09, val_acc=0.686]\n",
      "Epoch 8:  75%|███████▌  | 178/237 [00:00<00:00, 255.29it/s, loss=1.05, v_num=6, train_ce_loss=1.12, val_ce_loss=1.09, val_acc=0.686] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 237/237 [00:00<00:00, 253.39it/s, loss=1.05, v_num=6, train_ce_loss=1.12, val_ce_loss=1.08, val_acc=0.69] \n",
      "Epoch 9:  75%|███████▌  | 178/237 [00:00<00:00, 256.04it/s, loss=1.06, v_num=6, train_ce_loss=1.02, val_ce_loss=1.08, val_acc=0.69] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 237/237 [00:00<00:00, 253.74it/s, loss=1.06, v_num=6, train_ce_loss=1.02, val_ce_loss=1.08, val_acc=0.689]\n",
      "Epoch 10:  75%|███████▌  | 178/237 [00:00<00:00, 255.80it/s, loss=1.04, v_num=6, train_ce_loss=1.11, val_ce_loss=1.08, val_acc=0.689] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 237/237 [00:00<00:00, 254.08it/s, loss=1.04, v_num=6, train_ce_loss=1.11, val_ce_loss=1.07, val_acc=0.689]\n",
      "Epoch 11:  75%|███████▌  | 178/237 [00:00<00:00, 255.13it/s, loss=1.07, v_num=6, train_ce_loss=1.04, val_ce_loss=1.07, val_acc=0.689] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 237/237 [00:00<00:00, 252.27it/s, loss=1.07, v_num=6, train_ce_loss=1.04, val_ce_loss=1.07, val_acc=0.691]\n",
      "Epoch 12:  75%|███████▌  | 178/237 [00:00<00:00, 259.88it/s, loss=1.05, v_num=6, train_ce_loss=1.09, val_ce_loss=1.07, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 237/237 [00:00<00:00, 251.70it/s, loss=1.05, v_num=6, train_ce_loss=1.09, val_ce_loss=1.07, val_acc=0.689]\n",
      "Epoch 13:  75%|███████▌  | 178/237 [00:00<00:00, 259.61it/s, loss=1.04, v_num=6, train_ce_loss=0.964, val_ce_loss=1.07, val_acc=0.689]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 237/237 [00:00<00:00, 253.70it/s, loss=1.04, v_num=6, train_ce_loss=0.964, val_ce_loss=1.07, val_acc=0.689]\n",
      "Epoch 14:  75%|███████▌  | 178/237 [00:00<00:00, 264.34it/s, loss=1.05, v_num=6, train_ce_loss=1.17, val_ce_loss=1.07, val_acc=0.689] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 237/237 [00:00<00:00, 260.46it/s, loss=1.05, v_num=6, train_ce_loss=1.17, val_ce_loss=1.07, val_acc=0.692]\n",
      "Epoch 15:  75%|███████▌  | 178/237 [00:00<00:00, 255.28it/s, loss=1.04, v_num=6, train_ce_loss=1.04, val_ce_loss=1.07, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 237/237 [00:00<00:00, 250.72it/s, loss=1.04, v_num=6, train_ce_loss=1.04, val_ce_loss=1.07, val_acc=0.69] \n",
      "Epoch 16:  75%|███████▌  | 178/237 [00:00<00:00, 260.00it/s, loss=1.04, v_num=6, train_ce_loss=0.991, val_ce_loss=1.07, val_acc=0.69]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 237/237 [00:00<00:00, 256.88it/s, loss=1.04, v_num=6, train_ce_loss=0.991, val_ce_loss=1.07, val_acc=0.69]\n",
      "Epoch 17:  75%|███████▌  | 178/237 [00:00<00:00, 257.35it/s, loss=1.05, v_num=6, train_ce_loss=1.04, val_ce_loss=1.07, val_acc=0.69] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 237/237 [00:00<00:00, 255.39it/s, loss=1.05, v_num=6, train_ce_loss=1.04, val_ce_loss=1.07, val_acc=0.69]\n",
      "Epoch 18:  75%|███████▌  | 178/237 [00:00<00:00, 260.71it/s, loss=1.05, v_num=6, train_ce_loss=1.07, val_ce_loss=1.07, val_acc=0.69] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 237/237 [00:00<00:00, 256.44it/s, loss=1.05, v_num=6, train_ce_loss=1.07, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 19:  75%|███████▌  | 178/237 [00:00<00:00, 262.66it/s, loss=1.03, v_num=6, train_ce_loss=1.05, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 237/237 [00:00<00:00, 257.23it/s, loss=1.03, v_num=6, train_ce_loss=1.05, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 20:  75%|███████▌  | 178/237 [00:00<00:00, 249.40it/s, loss=1.05, v_num=6, train_ce_loss=1.05, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 237/237 [00:00<00:00, 246.41it/s, loss=1.05, v_num=6, train_ce_loss=1.05, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 21:  75%|███████▌  | 178/237 [00:00<00:00, 258.27it/s, loss=1.02, v_num=6, train_ce_loss=1.15, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 237/237 [00:00<00:00, 254.70it/s, loss=1.02, v_num=6, train_ce_loss=1.15, val_ce_loss=1.06, val_acc=0.693]\n",
      "Epoch 22:  75%|███████▌  | 178/237 [00:00<00:00, 265.74it/s, loss=1.04, v_num=6, train_ce_loss=0.995, val_ce_loss=1.06, val_acc=0.693]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|██████████| 237/237 [00:00<00:00, 257.08it/s, loss=1.04, v_num=6, train_ce_loss=0.995, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 23:  75%|███████▌  | 178/237 [00:00<00:00, 259.65it/s, loss=1.06, v_num=6, train_ce_loss=1.06, val_ce_loss=1.06, val_acc=0.691]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|██████████| 237/237 [00:00<00:00, 256.75it/s, loss=1.06, v_num=6, train_ce_loss=1.06, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 24:  75%|███████▌  | 178/237 [00:00<00:00, 260.96it/s, loss=1.04, v_num=6, train_ce_loss=1.16, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 237/237 [00:00<00:00, 256.71it/s, loss=1.04, v_num=6, train_ce_loss=1.16, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 25:  75%|███████▌  | 178/237 [00:00<00:00, 251.26it/s, loss=1.03, v_num=6, train_ce_loss=0.972, val_ce_loss=1.06, val_acc=0.691]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|██████████| 237/237 [00:00<00:00, 250.94it/s, loss=1.03, v_num=6, train_ce_loss=0.972, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 26:  75%|███████▌  | 178/237 [00:00<00:00, 250.49it/s, loss=1.04, v_num=6, train_ce_loss=1.05, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|██████████| 237/237 [00:00<00:00, 248.02it/s, loss=1.04, v_num=6, train_ce_loss=1.05, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 27:  75%|███████▌  | 178/237 [00:00<00:00, 256.63it/s, loss=1.03, v_num=6, train_ce_loss=0.942, val_ce_loss=1.06, val_acc=0.692]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|██████████| 237/237 [00:00<00:00, 254.78it/s, loss=1.03, v_num=6, train_ce_loss=0.942, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 28:  75%|███████▌  | 178/237 [00:00<00:00, 266.92it/s, loss=1.01, v_num=6, train_ce_loss=0.98, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|██████████| 237/237 [00:00<00:00, 261.89it/s, loss=1.01, v_num=6, train_ce_loss=0.98, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 29:  75%|███████▌  | 178/237 [00:00<00:00, 256.84it/s, loss=1.04, v_num=6, train_ce_loss=1.17, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|██████████| 237/237 [00:00<00:00, 254.93it/s, loss=1.04, v_num=6, train_ce_loss=1.17, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 30:  75%|███████▌  | 178/237 [00:00<00:00, 253.84it/s, loss=1.01, v_num=6, train_ce_loss=1.11, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|██████████| 237/237 [00:00<00:00, 249.41it/s, loss=1.01, v_num=6, train_ce_loss=1.11, val_ce_loss=1.06, val_acc=0.69] \n",
      "Epoch 31:  75%|███████▌  | 178/237 [00:00<00:00, 257.34it/s, loss=1.06, v_num=6, train_ce_loss=0.989, val_ce_loss=1.06, val_acc=0.69]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|██████████| 237/237 [00:00<00:00, 251.82it/s, loss=1.06, v_num=6, train_ce_loss=0.989, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 32:  75%|███████▌  | 178/237 [00:00<00:00, 260.54it/s, loss=1.04, v_num=6, train_ce_loss=1.09, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|██████████| 237/237 [00:00<00:00, 257.15it/s, loss=1.04, v_num=6, train_ce_loss=1.09, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 33:  75%|███████▌  | 178/237 [00:00<00:00, 253.02it/s, loss=1.03, v_num=6, train_ce_loss=1.07, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|██████████| 237/237 [00:00<00:00, 249.79it/s, loss=1.03, v_num=6, train_ce_loss=1.07, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 34:  75%|███████▌  | 178/237 [00:00<00:00, 252.64it/s, loss=1.04, v_num=6, train_ce_loss=1.02, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|██████████| 237/237 [00:00<00:00, 251.46it/s, loss=1.04, v_num=6, train_ce_loss=1.02, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 35:  75%|███████▌  | 178/237 [00:00<00:00, 258.15it/s, loss=0.999, v_num=6, train_ce_loss=0.995, val_ce_loss=1.06, val_acc=0.692]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|██████████| 237/237 [00:00<00:00, 252.38it/s, loss=0.999, v_num=6, train_ce_loss=0.995, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 36:  75%|███████▌  | 178/237 [00:00<00:00, 262.04it/s, loss=1.04, v_num=6, train_ce_loss=0.955, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|██████████| 237/237 [00:00<00:00, 258.27it/s, loss=1.04, v_num=6, train_ce_loss=0.955, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 37:  75%|███████▌  | 178/237 [00:00<00:00, 256.14it/s, loss=1.03, v_num=6, train_ce_loss=1.02, val_ce_loss=1.06, val_acc=0.691]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|██████████| 237/237 [00:00<00:00, 253.15it/s, loss=1.03, v_num=6, train_ce_loss=1.02, val_ce_loss=1.06, val_acc=0.693]\n",
      "Epoch 38:  75%|███████▌  | 178/237 [00:00<00:00, 262.61it/s, loss=1.04, v_num=6, train_ce_loss=1.07, val_ce_loss=1.06, val_acc=0.693]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|██████████| 237/237 [00:00<00:00, 257.71it/s, loss=1.04, v_num=6, train_ce_loss=1.07, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 39:  75%|███████▌  | 178/237 [00:00<00:00, 261.21it/s, loss=1.06, v_num=6, train_ce_loss=1.08, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|██████████| 237/237 [00:00<00:00, 257.87it/s, loss=1.06, v_num=6, train_ce_loss=1.08, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 40:  75%|███████▌  | 178/237 [00:00<00:00, 255.82it/s, loss=1.04, v_num=6, train_ce_loss=0.945, val_ce_loss=1.06, val_acc=0.692]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|██████████| 237/237 [00:00<00:00, 254.18it/s, loss=1.04, v_num=6, train_ce_loss=0.945, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 41:  75%|███████▌  | 178/237 [00:00<00:00, 255.67it/s, loss=1.05, v_num=6, train_ce_loss=1.23, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|██████████| 237/237 [00:00<00:00, 253.24it/s, loss=1.05, v_num=6, train_ce_loss=1.23, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 42:  75%|███████▌  | 178/237 [00:00<00:00, 261.29it/s, loss=1.04, v_num=6, train_ce_loss=0.988, val_ce_loss=1.06, val_acc=0.692]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|██████████| 237/237 [00:00<00:00, 254.35it/s, loss=1.04, v_num=6, train_ce_loss=0.988, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 43:  75%|███████▌  | 178/237 [00:00<00:00, 259.46it/s, loss=1.01, v_num=6, train_ce_loss=0.862, val_ce_loss=1.06, val_acc=0.691]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|██████████| 237/237 [00:00<00:00, 255.42it/s, loss=1.01, v_num=6, train_ce_loss=0.862, val_ce_loss=1.06, val_acc=0.693]\n",
      "Epoch 44:  75%|███████▌  | 178/237 [00:00<00:00, 260.42it/s, loss=1.04, v_num=6, train_ce_loss=1.18, val_ce_loss=1.06, val_acc=0.693]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|██████████| 237/237 [00:00<00:00, 251.85it/s, loss=1.04, v_num=6, train_ce_loss=1.18, val_ce_loss=1.05, val_acc=0.693]\n",
      "Epoch 45:  75%|███████▌  | 178/237 [00:00<00:00, 254.03it/s, loss=1.02, v_num=6, train_ce_loss=0.918, val_ce_loss=1.05, val_acc=0.693]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|██████████| 237/237 [00:00<00:00, 248.78it/s, loss=1.02, v_num=6, train_ce_loss=0.918, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 46:  75%|███████▌  | 178/237 [00:00<00:00, 257.44it/s, loss=1.02, v_num=6, train_ce_loss=0.881, val_ce_loss=1.06, val_acc=0.691]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|██████████| 237/237 [00:00<00:00, 252.55it/s, loss=1.02, v_num=6, train_ce_loss=0.881, val_ce_loss=1.06, val_acc=0.693]\n",
      "Epoch 47:  75%|███████▌  | 178/237 [00:00<00:00, 262.37it/s, loss=1.06, v_num=6, train_ce_loss=1.08, val_ce_loss=1.06, val_acc=0.693] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|██████████| 237/237 [00:00<00:00, 254.78it/s, loss=1.06, v_num=6, train_ce_loss=1.08, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 48:  75%|███████▌  | 178/237 [00:00<00:00, 260.47it/s, loss=1.03, v_num=6, train_ce_loss=1.02, val_ce_loss=1.06, val_acc=0.691] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|██████████| 237/237 [00:00<00:00, 257.82it/s, loss=1.03, v_num=6, train_ce_loss=1.02, val_ce_loss=1.06, val_acc=0.692]\n",
      "Epoch 49:  75%|███████▌  | 178/237 [00:00<00:00, 260.67it/s, loss=1.04, v_num=6, train_ce_loss=1.03, val_ce_loss=1.06, val_acc=0.692] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|██████████| 237/237 [00:00<00:00, 256.66it/s, loss=1.04, v_num=6, train_ce_loss=1.03, val_ce_loss=1.06, val_acc=0.691]\n",
      "Epoch 49: 100%|██████████| 237/237 [00:00<00:00, 255.97it/s, loss=1.04, v_num=6, train_ce_loss=1.03, val_ce_loss=1.06, val_acc=0.691]\n",
      "Testing: 100%|██████████| 59/59 [00:00<00:00, 244.75it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.6911, device='cuda:0'),\n",
      " 'test_ce_loss': tensor(1.0561, device='cuda:0'),\n",
      " 'test_loss': tensor(1.0561, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 95/95 [00:00<00:00, 312.43it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.6797, device='cuda:0'),\n",
      " 'test_ce_loss': tensor(1.1066, device='cuda:0'),\n",
      " 'test_loss': tensor(1.1066, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from diffusion_feature import preprocess\n",
    "# pytorch lighting based logistic regresison [easier to train, implemented already]\n",
    "from pl_bolts.models.regression import LogisticRegression\n",
    "\n",
    "def run_gpca(data_name, nhid, alpha, beta, nlayer, batch_size=256, max_epochs=50):\n",
    "    device = torch.device(\"cuda\"if torch.cuda.is_available() else \"cpu\")\n",
    "    data = load_ogb_data(data_name).to(device)\n",
    "\n",
    "    # add additonal embeddings\n",
    "#     x_additional = torch.cat([preprocess(data, 'diffusion', post_fix=data_name), \n",
    "#                               preprocess(data, 'spectral', post_fix=data_name)], dim=-1).to(device)\n",
    "#     data.x = torch.cat([data.x, x_additional], dim=-1)\n",
    "    \n",
    "    \n",
    "    # run gpca embedding\n",
    "    embeddings = gpca(data, nhid, alpha=alpha, beta=beta, nlayer=nlayer)\n",
    "\n",
    "    # step 1: create dataloader\n",
    "    data = data.to('cpu')\n",
    "    embeddings = embeddings.to('cpu')\n",
    "    \n",
    "    train_dataset = TabularDataset(embeddings[data.train_idx], data.y[data.train_idx])\n",
    "    valid_dataset = TabularDataset(embeddings[data.valid_idx], data.y[data.valid_idx])\n",
    "    test_dataset = TabularDataset(embeddings[data.test_idx], data.y[data.test_idx])\n",
    "    # DataLoader needs dataset in cpu when num_workers > 1\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True) \n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True) \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=False) \n",
    "\n",
    "    # step 2: create model and trainer\n",
    "    model = LogisticRegression(input_dim=nhid, num_classes=data.num_classes, \n",
    "                               learning_rate=0.001, l2_strength=5e-4)\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=max_epochs, callbacks=[ModelCheckpoint(monitor='val_acc',mode='max')])\n",
    "\n",
    "    # step 3: train and test\n",
    "    trainer.fit(model, train_loader, valid_loader)\n",
    "    valid_performance = trainer.test(model, test_dataloaders=valid_loader)\n",
    "    test_performance = trainer.test(model, test_dataloaders=test_loader)\n",
    "    \n",
    "    \n",
    "#     return valid_performance, test_performance\n",
    "    \n",
    "    \n",
    "# device = 'cpu'  # use when can not fit into GPU\n",
    "data_name = 'arxiv'\n",
    "nhid = 128\n",
    "alpha = 1\n",
    "beta = 0\n",
    "nlayer= 10\n",
    "run_gpca(data_name, nhid, alpha, beta, nlayer, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "1. Adding node2vec additional embeddings /cite the paper[Done]\n",
    "2. Think about nonlinearity and combining multiple layers embeddings\n",
    "3. Run on products\n",
    "4. Run multiple configurations\n",
    "5. Is there any other ways to leverage LP (label information)? \n",
    "6. How to setup initialization to see the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = torch.rand(10)\n",
    "m.nonzero().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OGB GCN implementation\n",
    "Test the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(device=0, dropout=0.5, epochs=500, hidden_channels=128, log_steps=1, lr=0.01, num_layers=3, runs=10, use_sage=False)\n",
      "Run: 01, Epoch: 01, Loss: 3.6930, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
      "Run: 01, Epoch: 02, Loss: 3.3665, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
      "Run: 01, Epoch: 03, Loss: 3.2360, Train: 17.32%, Valid: 24.96% Test: 22.75%\n",
      "Run: 01, Epoch: 04, Loss: 3.1203, Train: 15.32%, Valid: 17.78% Test: 23.91%\n",
      "Run: 01, Epoch: 05, Loss: 3.0866, Train: 23.14%, Valid: 20.10% Test: 24.64%\n",
      "Run: 01, Epoch: 06, Loss: 2.9869, Train: 17.99%, Valid: 7.74% Test: 5.89%\n",
      "Run: 01, Epoch: 07, Loss: 2.9366, Train: 26.20%, Valid: 27.59% Test: 25.25%\n",
      "Run: 01, Epoch: 08, Loss: 2.8721, Train: 28.45%, Valid: 30.34% Test: 27.18%\n",
      "Run: 01, Epoch: 09, Loss: 2.8110, Train: 29.72%, Valid: 30.88% Test: 27.56%\n",
      "Run: 01, Epoch: 10, Loss: 2.7263, Train: 32.89%, Valid: 32.81% Test: 29.52%\n",
      "Run: 01, Epoch: 11, Loss: 2.6298, Train: 39.19%, Valid: 42.19% Test: 41.13%\n",
      "Run: 01, Epoch: 12, Loss: 2.5528, Train: 42.45%, Valid: 45.60% Test: 44.97%\n",
      "Run: 01, Epoch: 13, Loss: 2.4703, Train: 39.90%, Valid: 39.96% Test: 37.59%\n",
      "Run: 01, Epoch: 14, Loss: 2.3928, Train: 40.54%, Valid: 39.46% Test: 36.76%\n",
      "Run: 01, Epoch: 15, Loss: 2.3208, Train: 44.51%, Valid: 43.71% Test: 41.01%\n",
      "Run: 01, Epoch: 16, Loss: 2.2447, Train: 46.91%, Valid: 48.33% Test: 45.36%\n",
      "Run: 01, Epoch: 17, Loss: 2.1888, Train: 46.62%, Valid: 48.64% Test: 47.06%\n",
      "Run: 01, Epoch: 18, Loss: 2.1243, Train: 47.58%, Valid: 50.05% Test: 49.05%\n",
      "Run: 01, Epoch: 19, Loss: 2.0654, Train: 49.94%, Valid: 52.10% Test: 50.74%\n",
      "Run: 01, Epoch: 20, Loss: 2.0188, Train: 51.10%, Valid: 52.71% Test: 51.55%\n",
      "Run: 01, Epoch: 21, Loss: 1.9715, Train: 51.91%, Valid: 53.16% Test: 51.58%\n",
      "Run: 01, Epoch: 22, Loss: 1.9280, Train: 52.55%, Valid: 53.51% Test: 51.04%\n",
      "Run: 01, Epoch: 23, Loss: 1.8872, Train: 53.12%, Valid: 54.96% Test: 53.47%\n",
      "Run: 01, Epoch: 24, Loss: 1.8577, Train: 53.62%, Valid: 56.34% Test: 56.06%\n",
      "Run: 01, Epoch: 25, Loss: 1.8319, Train: 55.25%, Valid: 57.76% Test: 56.89%\n",
      "Run: 01, Epoch: 26, Loss: 1.7987, Train: 56.12%, Valid: 56.80% Test: 54.98%\n",
      "Run: 01, Epoch: 27, Loss: 1.7684, Train: 56.65%, Valid: 57.44% Test: 55.86%\n",
      "Run: 01, Epoch: 28, Loss: 1.7404, Train: 57.00%, Valid: 58.67% Test: 57.54%\n",
      "Run: 01, Epoch: 29, Loss: 1.7108, Train: 57.35%, Valid: 59.02% Test: 58.04%\n",
      "Run: 01, Epoch: 30, Loss: 1.6916, Train: 58.27%, Valid: 59.22% Test: 57.86%\n",
      "Run: 01, Epoch: 31, Loss: 1.6643, Train: 58.91%, Valid: 59.67% Test: 58.17%\n",
      "Run: 01, Epoch: 32, Loss: 1.6401, Train: 59.19%, Valid: 60.20% Test: 58.65%\n",
      "Run: 01, Epoch: 33, Loss: 1.6116, Train: 59.57%, Valid: 60.84% Test: 59.76%\n",
      "Run: 01, Epoch: 34, Loss: 1.5866, Train: 60.07%, Valid: 61.02% Test: 59.96%\n",
      "Run: 01, Epoch: 35, Loss: 1.5772, Train: 60.44%, Valid: 61.12% Test: 59.74%\n",
      "Run: 01, Epoch: 36, Loss: 1.5508, Train: 60.66%, Valid: 61.47% Test: 60.24%\n",
      "Run: 01, Epoch: 37, Loss: 1.5396, Train: 60.93%, Valid: 61.84% Test: 60.78%\n",
      "Run: 01, Epoch: 38, Loss: 1.5209, Train: 61.17%, Valid: 61.92% Test: 60.62%\n",
      "Run: 01, Epoch: 39, Loss: 1.5068, Train: 61.37%, Valid: 62.37% Test: 61.41%\n",
      "Run: 01, Epoch: 40, Loss: 1.4962, Train: 61.54%, Valid: 62.64% Test: 61.74%\n",
      "Run: 01, Epoch: 41, Loss: 1.4821, Train: 61.75%, Valid: 62.69% Test: 61.73%\n",
      "Run: 01, Epoch: 42, Loss: 1.4704, Train: 61.98%, Valid: 62.61% Test: 61.21%\n",
      "Run: 01, Epoch: 43, Loss: 1.4524, Train: 62.20%, Valid: 63.06% Test: 62.17%\n",
      "Run: 01, Epoch: 44, Loss: 1.4493, Train: 62.32%, Valid: 63.46% Test: 62.80%\n",
      "Run: 01, Epoch: 45, Loss: 1.4361, Train: 62.43%, Valid: 62.79% Test: 61.13%\n",
      "Run: 01, Epoch: 46, Loss: 1.4285, Train: 62.64%, Valid: 63.35% Test: 62.45%\n",
      "Run: 01, Epoch: 47, Loss: 1.4210, Train: 62.80%, Valid: 63.67% Test: 63.05%\n",
      "Run: 01, Epoch: 48, Loss: 1.4055, Train: 62.99%, Valid: 63.78% Test: 63.17%\n",
      "Run: 01, Epoch: 49, Loss: 1.3981, Train: 63.24%, Valid: 64.01% Test: 63.19%\n",
      "Run: 01, Epoch: 50, Loss: 1.3923, Train: 63.41%, Valid: 63.83% Test: 62.72%\n",
      "Run: 01, Epoch: 51, Loss: 1.3847, Train: 63.59%, Valid: 64.35% Test: 63.50%\n",
      "Run: 01, Epoch: 52, Loss: 1.3788, Train: 63.68%, Valid: 64.40% Test: 63.62%\n",
      "Run: 01, Epoch: 53, Loss: 1.3691, Train: 63.82%, Valid: 64.48% Test: 63.49%\n",
      "Run: 01, Epoch: 54, Loss: 1.3625, Train: 63.97%, Valid: 64.67% Test: 63.77%\n",
      "Run: 01, Epoch: 55, Loss: 1.3590, Train: 64.13%, Valid: 64.73% Test: 63.91%\n",
      "Run: 01, Epoch: 56, Loss: 1.3496, Train: 64.25%, Valid: 64.89% Test: 64.08%\n",
      "Run: 01, Epoch: 57, Loss: 1.3425, Train: 64.45%, Valid: 65.25% Test: 64.63%\n",
      "Run: 01, Epoch: 58, Loss: 1.3365, Train: 64.58%, Valid: 65.00% Test: 64.06%\n",
      "Run: 01, Epoch: 59, Loss: 1.3309, Train: 64.63%, Valid: 64.94% Test: 63.85%\n",
      "Run: 01, Epoch: 60, Loss: 1.3239, Train: 64.80%, Valid: 65.39% Test: 64.51%\n",
      "Run: 01, Epoch: 61, Loss: 1.3250, Train: 64.99%, Valid: 65.62% Test: 64.65%\n",
      "Run: 01, Epoch: 62, Loss: 1.3153, Train: 65.08%, Valid: 65.53% Test: 64.42%\n",
      "Run: 01, Epoch: 63, Loss: 1.3107, Train: 65.23%, Valid: 65.75% Test: 64.90%\n",
      "Run: 01, Epoch: 64, Loss: 1.3077, Train: 65.29%, Valid: 65.65% Test: 64.61%\n",
      "Run: 01, Epoch: 65, Loss: 1.3023, Train: 65.49%, Valid: 65.94% Test: 64.97%\n",
      "Run: 01, Epoch: 66, Loss: 1.2969, Train: 65.62%, Valid: 66.02% Test: 65.00%\n",
      "Run: 01, Epoch: 67, Loss: 1.2910, Train: 65.71%, Valid: 66.18% Test: 65.22%\n",
      "Run: 01, Epoch: 68, Loss: 1.2877, Train: 65.90%, Valid: 66.28% Test: 65.34%\n",
      "Run: 01, Epoch: 69, Loss: 1.2853, Train: 65.93%, Valid: 66.45% Test: 65.53%\n",
      "Run: 01, Epoch: 70, Loss: 1.2828, Train: 65.98%, Valid: 66.40% Test: 65.51%\n",
      "Run: 01, Epoch: 71, Loss: 1.2729, Train: 66.14%, Valid: 66.48% Test: 65.23%\n",
      "Run: 01, Epoch: 72, Loss: 1.2709, Train: 66.15%, Valid: 66.34% Test: 65.19%\n",
      "Run: 01, Epoch: 73, Loss: 1.2675, Train: 66.27%, Valid: 66.75% Test: 65.87%\n",
      "Run: 01, Epoch: 74, Loss: 1.2671, Train: 66.39%, Valid: 66.82% Test: 66.08%\n",
      "Run: 01, Epoch: 75, Loss: 1.2581, Train: 66.52%, Valid: 66.74% Test: 65.75%\n",
      "Run: 01, Epoch: 76, Loss: 1.2560, Train: 66.55%, Valid: 66.94% Test: 65.93%\n",
      "Run: 01, Epoch: 77, Loss: 1.2549, Train: 66.59%, Valid: 66.88% Test: 65.81%\n",
      "Run: 01, Epoch: 78, Loss: 1.2476, Train: 66.66%, Valid: 67.04% Test: 65.88%\n",
      "Run: 01, Epoch: 79, Loss: 1.2446, Train: 66.76%, Valid: 67.22% Test: 66.25%\n",
      "Run: 01, Epoch: 80, Loss: 1.2405, Train: 66.80%, Valid: 67.34% Test: 66.35%\n",
      "Run: 01, Epoch: 81, Loss: 1.2403, Train: 66.90%, Valid: 67.14% Test: 65.90%\n",
      "Run: 01, Epoch: 82, Loss: 1.2352, Train: 67.06%, Valid: 67.60% Test: 66.78%\n",
      "Run: 01, Epoch: 83, Loss: 1.2323, Train: 67.04%, Valid: 67.36% Test: 66.39%\n",
      "Run: 01, Epoch: 84, Loss: 1.2328, Train: 67.10%, Valid: 67.40% Test: 66.31%\n",
      "Run: 01, Epoch: 85, Loss: 1.2284, Train: 67.16%, Valid: 67.68% Test: 66.57%\n",
      "Run: 01, Epoch: 86, Loss: 1.2211, Train: 67.22%, Valid: 67.76% Test: 66.66%\n",
      "Run: 01, Epoch: 87, Loss: 1.2215, Train: 67.32%, Valid: 67.77% Test: 66.66%\n",
      "Run: 01, Epoch: 88, Loss: 1.2194, Train: 67.38%, Valid: 67.66% Test: 66.55%\n",
      "Run: 01, Epoch: 89, Loss: 1.2153, Train: 67.44%, Valid: 68.02% Test: 67.33%\n",
      "Run: 01, Epoch: 90, Loss: 1.2115, Train: 67.52%, Valid: 67.61% Test: 66.54%\n",
      "Run: 01, Epoch: 91, Loss: 1.2103, Train: 67.59%, Valid: 67.39% Test: 66.12%\n",
      "Run: 01, Epoch: 92, Loss: 1.2068, Train: 67.57%, Valid: 68.19% Test: 67.53%\n",
      "Run: 01, Epoch: 93, Loss: 1.2050, Train: 67.65%, Valid: 68.12% Test: 67.16%\n",
      "Run: 01, Epoch: 94, Loss: 1.2018, Train: 67.78%, Valid: 68.07% Test: 66.92%\n",
      "Run: 01, Epoch: 95, Loss: 1.2017, Train: 67.84%, Valid: 68.22% Test: 67.19%\n",
      "Run: 01, Epoch: 96, Loss: 1.1998, Train: 67.90%, Valid: 68.17% Test: 67.10%\n",
      "Run: 01, Epoch: 97, Loss: 1.1975, Train: 67.90%, Valid: 68.28% Test: 67.16%\n",
      "Run: 01, Epoch: 98, Loss: 1.1948, Train: 68.01%, Valid: 68.22% Test: 67.14%\n",
      "Run: 01, Epoch: 99, Loss: 1.1891, Train: 68.11%, Valid: 68.46% Test: 67.61%\n",
      "Run: 01, Epoch: 100, Loss: 1.1909, Train: 68.11%, Valid: 68.36% Test: 67.38%\n",
      "Run: 01, Epoch: 101, Loss: 1.1883, Train: 68.05%, Valid: 67.78% Test: 66.49%\n",
      "Run: 01, Epoch: 102, Loss: 1.1869, Train: 68.02%, Valid: 68.60% Test: 68.09%\n",
      "Run: 01, Epoch: 103, Loss: 1.1894, Train: 68.21%, Valid: 68.12% Test: 67.03%\n",
      "Run: 01, Epoch: 104, Loss: 1.1793, Train: 68.32%, Valid: 68.40% Test: 67.36%\n",
      "Run: 01, Epoch: 105, Loss: 1.1799, Train: 68.33%, Valid: 68.69% Test: 67.87%\n",
      "Run: 01, Epoch: 106, Loss: 1.1764, Train: 68.32%, Valid: 68.48% Test: 67.15%\n",
      "Run: 01, Epoch: 107, Loss: 1.1716, Train: 68.30%, Valid: 68.53% Test: 67.58%\n",
      "Run: 01, Epoch: 108, Loss: 1.1753, Train: 68.38%, Valid: 68.33% Test: 67.14%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 109, Loss: 1.1697, Train: 68.49%, Valid: 68.74% Test: 67.75%\n",
      "Run: 01, Epoch: 110, Loss: 1.1719, Train: 68.53%, Valid: 68.75% Test: 67.88%\n",
      "Run: 01, Epoch: 111, Loss: 1.1696, Train: 68.49%, Valid: 68.38% Test: 67.11%\n",
      "Run: 01, Epoch: 112, Loss: 1.1681, Train: 68.63%, Valid: 68.79% Test: 68.08%\n",
      "Run: 01, Epoch: 113, Loss: 1.1661, Train: 68.66%, Valid: 68.65% Test: 67.37%\n",
      "Run: 01, Epoch: 114, Loss: 1.1632, Train: 68.65%, Valid: 68.52% Test: 67.31%\n",
      "Run: 01, Epoch: 115, Loss: 1.1643, Train: 68.66%, Valid: 68.89% Test: 68.01%\n",
      "Run: 01, Epoch: 116, Loss: 1.1619, Train: 68.67%, Valid: 68.46% Test: 67.19%\n",
      "Run: 01, Epoch: 117, Loss: 1.1550, Train: 68.70%, Valid: 68.82% Test: 68.12%\n",
      "Run: 01, Epoch: 118, Loss: 1.1565, Train: 68.88%, Valid: 68.72% Test: 67.49%\n",
      "Run: 01, Epoch: 119, Loss: 1.1549, Train: 68.90%, Valid: 68.97% Test: 67.73%\n",
      "Run: 01, Epoch: 120, Loss: 1.1512, Train: 68.86%, Valid: 69.00% Test: 68.41%\n",
      "Run: 01, Epoch: 121, Loss: 1.1488, Train: 68.87%, Valid: 68.43% Test: 66.91%\n",
      "Run: 01, Epoch: 122, Loss: 1.1515, Train: 68.97%, Valid: 69.16% Test: 68.24%\n",
      "Run: 01, Epoch: 123, Loss: 1.1467, Train: 69.01%, Valid: 68.96% Test: 67.97%\n",
      "Run: 01, Epoch: 124, Loss: 1.1457, Train: 69.08%, Valid: 68.79% Test: 67.43%\n",
      "Run: 01, Epoch: 125, Loss: 1.1445, Train: 69.10%, Valid: 68.97% Test: 68.21%\n",
      "Run: 01, Epoch: 126, Loss: 1.1431, Train: 69.12%, Valid: 68.85% Test: 67.70%\n",
      "Run: 01, Epoch: 127, Loss: 1.1442, Train: 69.16%, Valid: 69.01% Test: 67.71%\n",
      "Run: 01, Epoch: 128, Loss: 1.1423, Train: 69.03%, Valid: 69.11% Test: 68.20%\n",
      "Run: 01, Epoch: 129, Loss: 1.1397, Train: 69.18%, Valid: 68.85% Test: 67.62%\n",
      "Run: 01, Epoch: 130, Loss: 1.1399, Train: 69.19%, Valid: 69.25% Test: 68.02%\n",
      "Run: 01, Epoch: 131, Loss: 1.1417, Train: 69.19%, Valid: 69.08% Test: 68.24%\n",
      "Run: 01, Epoch: 132, Loss: 1.1335, Train: 69.27%, Valid: 68.79% Test: 67.35%\n",
      "Run: 01, Epoch: 133, Loss: 1.1361, Train: 69.38%, Valid: 69.27% Test: 67.94%\n",
      "Run: 01, Epoch: 134, Loss: 1.1368, Train: 69.25%, Valid: 69.30% Test: 68.65%\n",
      "Run: 01, Epoch: 135, Loss: 1.1343, Train: 69.33%, Valid: 68.96% Test: 67.68%\n",
      "Run: 01, Epoch: 136, Loss: 1.1280, Train: 69.40%, Valid: 68.95% Test: 67.48%\n",
      "Run: 01, Epoch: 137, Loss: 1.1314, Train: 69.34%, Valid: 69.38% Test: 68.66%\n",
      "Run: 01, Epoch: 138, Loss: 1.1342, Train: 69.47%, Valid: 68.97% Test: 67.75%\n",
      "Run: 01, Epoch: 139, Loss: 1.1220, Train: 69.45%, Valid: 68.98% Test: 67.61%\n",
      "Run: 01, Epoch: 140, Loss: 1.1264, Train: 69.45%, Valid: 69.23% Test: 68.30%\n",
      "Run: 01, Epoch: 141, Loss: 1.1305, Train: 69.60%, Valid: 69.38% Test: 68.51%\n",
      "Run: 01, Epoch: 142, Loss: 1.1266, Train: 69.53%, Valid: 68.83% Test: 67.36%\n",
      "Run: 01, Epoch: 143, Loss: 1.1255, Train: 69.55%, Valid: 69.21% Test: 68.19%\n",
      "Run: 01, Epoch: 144, Loss: 1.1237, Train: 69.56%, Valid: 69.52% Test: 68.67%\n",
      "Run: 01, Epoch: 145, Loss: 1.1207, Train: 69.58%, Valid: 68.92% Test: 67.46%\n",
      "Run: 01, Epoch: 146, Loss: 1.1250, Train: 69.64%, Valid: 69.43% Test: 68.64%\n",
      "Run: 01, Epoch: 147, Loss: 1.1214, Train: 69.64%, Valid: 69.07% Test: 67.84%\n",
      "Run: 01, Epoch: 148, Loss: 1.1158, Train: 69.72%, Valid: 69.38% Test: 68.07%\n",
      "Run: 01, Epoch: 149, Loss: 1.1178, Train: 69.72%, Valid: 69.51% Test: 68.64%\n",
      "Run: 01, Epoch: 150, Loss: 1.1153, Train: 69.73%, Valid: 69.27% Test: 67.77%\n",
      "Run: 01, Epoch: 151, Loss: 1.1175, Train: 69.74%, Valid: 69.48% Test: 68.20%\n",
      "Run: 01, Epoch: 152, Loss: 1.1140, Train: 69.71%, Valid: 69.49% Test: 68.77%\n",
      "Run: 01, Epoch: 153, Loss: 1.1164, Train: 69.73%, Valid: 68.89% Test: 67.37%\n",
      "Run: 01, Epoch: 154, Loss: 1.1159, Train: 69.75%, Valid: 69.37% Test: 68.60%\n",
      "Run: 01, Epoch: 155, Loss: 1.1118, Train: 69.82%, Valid: 69.33% Test: 68.11%\n",
      "Run: 01, Epoch: 156, Loss: 1.1134, Train: 69.89%, Valid: 69.51% Test: 68.04%\n",
      "Run: 01, Epoch: 157, Loss: 1.1095, Train: 69.88%, Valid: 69.55% Test: 68.70%\n",
      "Run: 01, Epoch: 158, Loss: 1.1060, Train: 69.87%, Valid: 69.48% Test: 68.44%\n",
      "Run: 01, Epoch: 159, Loss: 1.1116, Train: 69.92%, Valid: 69.50% Test: 68.27%\n",
      "Run: 01, Epoch: 160, Loss: 1.1068, Train: 69.99%, Valid: 69.61% Test: 68.63%\n",
      "Run: 01, Epoch: 161, Loss: 1.1065, Train: 69.87%, Valid: 69.36% Test: 68.23%\n",
      "Run: 01, Epoch: 162, Loss: 1.1081, Train: 69.98%, Valid: 69.68% Test: 68.43%\n",
      "Run: 01, Epoch: 163, Loss: 1.1077, Train: 70.04%, Valid: 69.73% Test: 68.65%\n",
      "Run: 01, Epoch: 164, Loss: 1.1039, Train: 69.97%, Valid: 69.35% Test: 68.12%\n",
      "Run: 01, Epoch: 165, Loss: 1.1043, Train: 69.99%, Valid: 69.68% Test: 68.42%\n",
      "Run: 01, Epoch: 166, Loss: 1.1042, Train: 70.10%, Valid: 69.78% Test: 68.48%\n",
      "Run: 01, Epoch: 167, Loss: 1.0990, Train: 70.07%, Valid: 69.60% Test: 68.31%\n",
      "Run: 01, Epoch: 168, Loss: 1.0974, Train: 70.08%, Valid: 69.68% Test: 68.21%\n",
      "Run: 01, Epoch: 169, Loss: 1.0989, Train: 70.19%, Valid: 69.76% Test: 68.42%\n",
      "Run: 01, Epoch: 170, Loss: 1.0943, Train: 70.15%, Valid: 69.78% Test: 68.89%\n",
      "Run: 01, Epoch: 171, Loss: 1.1006, Train: 70.16%, Valid: 69.60% Test: 68.30%\n",
      "Run: 01, Epoch: 172, Loss: 1.0961, Train: 70.25%, Valid: 69.97% Test: 68.87%\n",
      "Run: 01, Epoch: 173, Loss: 1.0973, Train: 70.23%, Valid: 69.89% Test: 68.69%\n",
      "Run: 01, Epoch: 174, Loss: 1.0995, Train: 70.24%, Valid: 69.61% Test: 68.23%\n",
      "Run: 01, Epoch: 175, Loss: 1.0966, Train: 70.24%, Valid: 69.80% Test: 68.43%\n",
      "Run: 01, Epoch: 176, Loss: 1.0955, Train: 70.30%, Valid: 69.95% Test: 68.89%\n",
      "Run: 01, Epoch: 177, Loss: 1.0925, Train: 70.30%, Valid: 69.61% Test: 68.12%\n",
      "Run: 01, Epoch: 178, Loss: 1.0924, Train: 70.25%, Valid: 69.94% Test: 68.97%\n",
      "Run: 01, Epoch: 179, Loss: 1.0954, Train: 70.28%, Valid: 69.80% Test: 68.55%\n",
      "Run: 01, Epoch: 180, Loss: 1.0906, Train: 70.35%, Valid: 70.00% Test: 68.85%\n",
      "Run: 01, Epoch: 181, Loss: 1.0889, Train: 70.34%, Valid: 69.86% Test: 68.46%\n",
      "Run: 01, Epoch: 182, Loss: 1.0897, Train: 70.42%, Valid: 69.87% Test: 68.56%\n",
      "Run: 01, Epoch: 183, Loss: 1.0897, Train: 70.38%, Valid: 70.02% Test: 69.07%\n",
      "Run: 01, Epoch: 184, Loss: 1.0888, Train: 70.45%, Valid: 69.68% Test: 68.02%\n",
      "Run: 01, Epoch: 185, Loss: 1.0885, Train: 70.48%, Valid: 69.86% Test: 68.89%\n",
      "Run: 01, Epoch: 186, Loss: 1.0867, Train: 70.52%, Valid: 70.16% Test: 69.11%\n",
      "Run: 01, Epoch: 187, Loss: 1.0883, Train: 70.39%, Valid: 69.55% Test: 67.84%\n",
      "Run: 01, Epoch: 188, Loss: 1.0849, Train: 70.39%, Valid: 69.98% Test: 69.34%\n",
      "Run: 01, Epoch: 189, Loss: 1.0890, Train: 70.46%, Valid: 69.67% Test: 68.21%\n",
      "Run: 01, Epoch: 190, Loss: 1.0840, Train: 70.49%, Valid: 70.11% Test: 68.86%\n",
      "Run: 01, Epoch: 191, Loss: 1.0825, Train: 70.45%, Valid: 69.95% Test: 68.55%\n",
      "Run: 01, Epoch: 192, Loss: 1.0852, Train: 70.51%, Valid: 70.08% Test: 68.73%\n",
      "Run: 01, Epoch: 193, Loss: 1.0830, Train: 70.59%, Valid: 70.22% Test: 68.89%\n",
      "Run: 01, Epoch: 194, Loss: 1.0812, Train: 70.50%, Valid: 69.85% Test: 68.44%\n",
      "Run: 01, Epoch: 195, Loss: 1.0780, Train: 70.57%, Valid: 70.07% Test: 68.90%\n",
      "Run: 01, Epoch: 196, Loss: 1.0822, Train: 70.56%, Valid: 70.14% Test: 68.70%\n",
      "Run: 01, Epoch: 197, Loss: 1.0794, Train: 70.61%, Valid: 70.25% Test: 69.05%\n",
      "Run: 01, Epoch: 198, Loss: 1.0790, Train: 70.62%, Valid: 70.04% Test: 68.68%\n",
      "Run: 01, Epoch: 199, Loss: 1.0753, Train: 70.63%, Valid: 69.99% Test: 68.75%\n",
      "Run: 01, Epoch: 200, Loss: 1.0800, Train: 70.61%, Valid: 70.12% Test: 68.85%\n",
      "Run: 01, Epoch: 201, Loss: 1.0766, Train: 70.63%, Valid: 70.14% Test: 68.78%\n",
      "Run: 01, Epoch: 202, Loss: 1.0743, Train: 70.68%, Valid: 69.98% Test: 68.61%\n",
      "Run: 01, Epoch: 203, Loss: 1.0746, Train: 70.66%, Valid: 70.20% Test: 69.33%\n",
      "Run: 01, Epoch: 204, Loss: 1.0767, Train: 70.66%, Valid: 69.97% Test: 68.41%\n",
      "Run: 01, Epoch: 205, Loss: 1.0783, Train: 70.76%, Valid: 70.24% Test: 69.14%\n",
      "Run: 01, Epoch: 206, Loss: 1.0738, Train: 70.72%, Valid: 70.24% Test: 69.26%\n",
      "Run: 01, Epoch: 207, Loss: 1.0752, Train: 70.70%, Valid: 69.69% Test: 67.95%\n",
      "Run: 01, Epoch: 208, Loss: 1.0743, Train: 70.76%, Valid: 70.21% Test: 69.08%\n",
      "Run: 01, Epoch: 209, Loss: 1.0752, Train: 70.79%, Valid: 70.16% Test: 68.89%\n",
      "Run: 01, Epoch: 210, Loss: 1.0747, Train: 70.77%, Valid: 69.98% Test: 68.40%\n",
      "Run: 01, Epoch: 211, Loss: 1.0731, Train: 70.83%, Valid: 70.34% Test: 69.27%\n",
      "Run: 01, Epoch: 212, Loss: 1.0688, Train: 70.78%, Valid: 70.05% Test: 68.57%\n",
      "Run: 01, Epoch: 213, Loss: 1.0721, Train: 70.81%, Valid: 70.17% Test: 69.08%\n",
      "Run: 01, Epoch: 214, Loss: 1.0701, Train: 70.79%, Valid: 70.01% Test: 68.59%\n",
      "Run: 01, Epoch: 215, Loss: 1.0729, Train: 70.82%, Valid: 70.34% Test: 69.28%\n",
      "Run: 01, Epoch: 216, Loss: 1.0673, Train: 70.87%, Valid: 70.21% Test: 68.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 217, Loss: 1.0655, Train: 70.83%, Valid: 70.14% Test: 68.85%\n",
      "Run: 01, Epoch: 218, Loss: 1.0659, Train: 70.83%, Valid: 70.14% Test: 68.81%\n",
      "Run: 01, Epoch: 219, Loss: 1.0643, Train: 70.84%, Valid: 70.29% Test: 69.25%\n",
      "Run: 01, Epoch: 220, Loss: 1.0693, Train: 70.86%, Valid: 70.17% Test: 68.99%\n",
      "Run: 01, Epoch: 221, Loss: 1.0649, Train: 70.91%, Valid: 70.36% Test: 69.24%\n",
      "Run: 01, Epoch: 222, Loss: 1.0645, Train: 70.85%, Valid: 70.13% Test: 68.69%\n",
      "Run: 01, Epoch: 223, Loss: 1.0630, Train: 70.84%, Valid: 69.96% Test: 68.46%\n",
      "Run: 01, Epoch: 224, Loss: 1.0694, Train: 70.85%, Valid: 70.22% Test: 69.55%\n",
      "Run: 01, Epoch: 225, Loss: 1.0690, Train: 70.95%, Valid: 69.77% Test: 68.06%\n",
      "Run: 01, Epoch: 226, Loss: 1.0647, Train: 70.95%, Valid: 70.23% Test: 68.88%\n",
      "Run: 01, Epoch: 227, Loss: 1.0653, Train: 70.96%, Valid: 70.35% Test: 69.49%\n",
      "Run: 01, Epoch: 228, Loss: 1.0646, Train: 70.87%, Valid: 69.61% Test: 67.84%\n",
      "Run: 01, Epoch: 229, Loss: 1.0654, Train: 70.89%, Valid: 70.41% Test: 69.67%\n",
      "Run: 01, Epoch: 230, Loss: 1.0631, Train: 71.03%, Valid: 70.13% Test: 68.66%\n",
      "Run: 01, Epoch: 231, Loss: 1.0649, Train: 70.94%, Valid: 69.92% Test: 68.31%\n",
      "Run: 01, Epoch: 232, Loss: 1.0657, Train: 70.89%, Valid: 70.41% Test: 69.83%\n",
      "Run: 01, Epoch: 233, Loss: 1.0677, Train: 70.96%, Valid: 69.36% Test: 67.46%\n",
      "Run: 01, Epoch: 234, Loss: 1.0617, Train: 71.04%, Valid: 70.29% Test: 69.10%\n",
      "Run: 01, Epoch: 235, Loss: 1.0601, Train: 70.96%, Valid: 70.30% Test: 69.66%\n",
      "Run: 01, Epoch: 236, Loss: 1.0655, Train: 70.92%, Valid: 69.39% Test: 67.50%\n",
      "Run: 01, Epoch: 237, Loss: 1.0645, Train: 71.07%, Valid: 70.50% Test: 69.29%\n",
      "Run: 01, Epoch: 238, Loss: 1.0590, Train: 71.07%, Valid: 70.54% Test: 69.76%\n",
      "Run: 01, Epoch: 239, Loss: 1.0599, Train: 71.00%, Valid: 69.65% Test: 67.96%\n",
      "Run: 01, Epoch: 240, Loss: 1.0570, Train: 71.10%, Valid: 70.44% Test: 69.31%\n",
      "Run: 01, Epoch: 241, Loss: 1.0555, Train: 71.09%, Valid: 70.51% Test: 69.54%\n",
      "Run: 01, Epoch: 242, Loss: 1.0577, Train: 71.06%, Valid: 70.00% Test: 68.37%\n",
      "Run: 01, Epoch: 243, Loss: 1.0612, Train: 71.17%, Valid: 70.50% Test: 69.42%\n",
      "Run: 01, Epoch: 244, Loss: 1.0474, Train: 71.14%, Valid: 70.45% Test: 69.36%\n",
      "Run: 01, Epoch: 245, Loss: 1.0553, Train: 71.12%, Valid: 70.04% Test: 68.43%\n",
      "Run: 01, Epoch: 246, Loss: 1.0546, Train: 71.15%, Valid: 70.55% Test: 69.59%\n",
      "Run: 01, Epoch: 247, Loss: 1.0558, Train: 71.18%, Valid: 70.36% Test: 69.04%\n",
      "Run: 01, Epoch: 248, Loss: 1.0560, Train: 71.16%, Valid: 70.14% Test: 68.64%\n",
      "Run: 01, Epoch: 249, Loss: 1.0540, Train: 71.21%, Valid: 70.29% Test: 69.05%\n",
      "Run: 01, Epoch: 250, Loss: 1.0499, Train: 71.20%, Valid: 70.58% Test: 69.38%\n",
      "Run: 01, Epoch: 251, Loss: 1.0507, Train: 71.22%, Valid: 70.46% Test: 69.04%\n",
      "Run: 01, Epoch: 252, Loss: 1.0509, Train: 71.20%, Valid: 70.53% Test: 69.32%\n",
      "Run: 01, Epoch: 253, Loss: 1.0496, Train: 71.26%, Valid: 70.56% Test: 69.65%\n",
      "Run: 01, Epoch: 254, Loss: 1.0512, Train: 71.25%, Valid: 70.19% Test: 68.60%\n",
      "Run: 01, Epoch: 255, Loss: 1.0529, Train: 71.30%, Valid: 70.53% Test: 69.44%\n",
      "Run: 01, Epoch: 256, Loss: 1.0460, Train: 71.22%, Valid: 70.56% Test: 69.52%\n",
      "Run: 01, Epoch: 257, Loss: 1.0496, Train: 71.28%, Valid: 69.91% Test: 68.11%\n",
      "Run: 01, Epoch: 258, Loss: 1.0496, Train: 71.32%, Valid: 70.64% Test: 69.51%\n",
      "Run: 01, Epoch: 259, Loss: 1.0494, Train: 71.29%, Valid: 70.69% Test: 69.79%\n",
      "Run: 01, Epoch: 260, Loss: 1.0488, Train: 71.30%, Valid: 69.94% Test: 68.16%\n",
      "Run: 01, Epoch: 261, Loss: 1.0480, Train: 71.31%, Valid: 70.59% Test: 69.69%\n",
      "Run: 01, Epoch: 262, Loss: 1.0488, Train: 71.34%, Valid: 70.65% Test: 69.54%\n",
      "Run: 01, Epoch: 263, Loss: 1.0472, Train: 71.34%, Valid: 70.18% Test: 68.49%\n",
      "Run: 01, Epoch: 264, Loss: 1.0489, Train: 71.29%, Valid: 70.47% Test: 69.40%\n",
      "Run: 01, Epoch: 265, Loss: 1.0454, Train: 71.38%, Valid: 70.54% Test: 69.34%\n",
      "Run: 01, Epoch: 266, Loss: 1.0426, Train: 71.38%, Valid: 70.21% Test: 68.46%\n",
      "Run: 01, Epoch: 267, Loss: 1.0494, Train: 71.40%, Valid: 70.62% Test: 69.63%\n",
      "Run: 01, Epoch: 268, Loss: 1.0440, Train: 71.42%, Valid: 70.60% Test: 69.41%\n",
      "Run: 01, Epoch: 269, Loss: 1.0468, Train: 71.39%, Valid: 70.22% Test: 68.52%\n",
      "Run: 01, Epoch: 270, Loss: 1.0451, Train: 71.38%, Valid: 70.69% Test: 69.95%\n",
      "Run: 01, Epoch: 271, Loss: 1.0450, Train: 71.48%, Valid: 70.45% Test: 69.09%\n",
      "Run: 01, Epoch: 272, Loss: 1.0421, Train: 71.46%, Valid: 70.35% Test: 68.79%\n",
      "Run: 01, Epoch: 273, Loss: 1.0413, Train: 71.43%, Valid: 70.66% Test: 69.84%\n",
      "Run: 01, Epoch: 274, Loss: 1.0447, Train: 71.47%, Valid: 70.39% Test: 68.74%\n",
      "Run: 01, Epoch: 275, Loss: 1.0424, Train: 71.46%, Valid: 70.56% Test: 69.34%\n",
      "Run: 01, Epoch: 276, Loss: 1.0413, Train: 71.48%, Valid: 70.68% Test: 69.59%\n",
      "Run: 01, Epoch: 277, Loss: 1.0414, Train: 71.53%, Valid: 70.51% Test: 69.02%\n",
      "Run: 01, Epoch: 278, Loss: 1.0394, Train: 71.55%, Valid: 70.56% Test: 69.39%\n",
      "Run: 01, Epoch: 279, Loss: 1.0411, Train: 71.50%, Valid: 70.43% Test: 69.06%\n",
      "Run: 01, Epoch: 280, Loss: 1.0377, Train: 71.60%, Valid: 70.49% Test: 69.20%\n",
      "Run: 01, Epoch: 281, Loss: 1.0373, Train: 71.53%, Valid: 70.69% Test: 69.43%\n",
      "Run: 01, Epoch: 282, Loss: 1.0402, Train: 71.54%, Valid: 70.40% Test: 68.85%\n",
      "Run: 01, Epoch: 283, Loss: 1.0344, Train: 71.57%, Valid: 70.70% Test: 69.60%\n",
      "Run: 01, Epoch: 284, Loss: 1.0382, Train: 71.57%, Valid: 70.59% Test: 69.28%\n",
      "Run: 01, Epoch: 285, Loss: 1.0380, Train: 71.63%, Valid: 70.63% Test: 69.47%\n",
      "Run: 01, Epoch: 286, Loss: 1.0361, Train: 71.50%, Valid: 70.43% Test: 69.29%\n",
      "Run: 01, Epoch: 287, Loss: 1.0387, Train: 71.58%, Valid: 70.54% Test: 68.77%\n",
      "Run: 01, Epoch: 288, Loss: 1.0353, Train: 71.52%, Valid: 70.80% Test: 69.95%\n",
      "Run: 01, Epoch: 289, Loss: 1.0373, Train: 71.56%, Valid: 70.39% Test: 69.02%\n",
      "Run: 01, Epoch: 290, Loss: 1.0391, Train: 71.65%, Valid: 70.43% Test: 68.99%\n",
      "Run: 01, Epoch: 291, Loss: 1.0367, Train: 71.65%, Valid: 70.82% Test: 69.83%\n",
      "Run: 01, Epoch: 292, Loss: 1.0364, Train: 71.66%, Valid: 70.72% Test: 69.63%\n",
      "Run: 01, Epoch: 293, Loss: 1.0355, Train: 71.65%, Valid: 70.45% Test: 69.10%\n",
      "Run: 01, Epoch: 294, Loss: 1.0350, Train: 71.63%, Valid: 70.73% Test: 69.53%\n",
      "Run: 01, Epoch: 295, Loss: 1.0353, Train: 71.70%, Valid: 70.74% Test: 69.50%\n",
      "Run: 01, Epoch: 296, Loss: 1.0346, Train: 71.68%, Valid: 70.24% Test: 68.62%\n",
      "Run: 01, Epoch: 297, Loss: 1.0343, Train: 71.65%, Valid: 70.77% Test: 69.69%\n",
      "Run: 01, Epoch: 298, Loss: 1.0368, Train: 71.71%, Valid: 70.78% Test: 69.54%\n",
      "Run: 01, Epoch: 299, Loss: 1.0342, Train: 71.70%, Valid: 70.36% Test: 68.95%\n",
      "Run: 01, Epoch: 300, Loss: 1.0356, Train: 71.62%, Valid: 70.76% Test: 69.63%\n",
      "Run: 01, Epoch: 301, Loss: 1.0333, Train: 71.70%, Valid: 70.56% Test: 68.96%\n",
      "Run: 01, Epoch: 302, Loss: 1.0340, Train: 71.69%, Valid: 70.45% Test: 69.01%\n",
      "Run: 01, Epoch: 303, Loss: 1.0315, Train: 71.70%, Valid: 70.77% Test: 69.71%\n",
      "Run: 01, Epoch: 304, Loss: 1.0320, Train: 71.68%, Valid: 70.69% Test: 69.37%\n",
      "Run: 01, Epoch: 305, Loss: 1.0303, Train: 71.74%, Valid: 70.59% Test: 69.24%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ec020f7f0f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-ec020f7f0f43>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-ec020f7f0f43>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, train_idx, optimizer)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "#             x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='OGBN-Arxiv (GNN)')\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "    parser.add_argument('--log_steps', type=int, default=1)\n",
    "    parser.add_argument('--use_sage', action='store_true')\n",
    "    parser.add_argument('--num_layers', type=int, default=3)\n",
    "    parser.add_argument('--hidden_channels', type=int, default=128)\n",
    "    parser.add_argument('--dropout', type=float, default=0.5)\n",
    "    parser.add_argument('--lr', type=float, default=0.01)\n",
    "    parser.add_argument('--epochs', type=int, default=500)\n",
    "    parser.add_argument('--runs', type=int, default=10)\n",
    "    args = parser.parse_args('')\n",
    "    print(args)\n",
    "\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
    "                                     transform=T.ToSparseTensor())\n",
    "\n",
    "    data = dataset[0]\n",
    "#     data.edge_index = to_undirected(data.edge_index, data.num_nodes)\n",
    "#     data.adj_t = data.edge_index\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    data = data.to(device)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    if args.use_sage:\n",
    "        model = SAGE(data.num_features, args.hidden_channels,\n",
    "                     dataset.num_classes, args.num_layers,\n",
    "                     args.dropout).to(device)\n",
    "    else:\n",
    "        model = GCN(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, split_idx, evaluator)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.6]",
   "language": "python",
   "name": "conda-env-pytorch1.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
